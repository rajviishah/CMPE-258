{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMPE 258 - Assignment 3 - Part d.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Write  a colab  pytorch lightening version**"
      ],
      "metadata": {
        "id": "ZmDHFNYGOwK2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoRwftaymEt1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e2a5e8-24ff-4e8f-b683-ebac1c7bf341"
      },
      "source": [
        "!pip install pytorch-lightning"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.7/dist-packages (1.5.10)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.18.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.8.0)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.10.0.2)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2022.2.0)\n",
            "Requirement already satisfied: setuptools==59.5.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (59.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.63.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.3)\n",
            "Requirement already satisfied: pyDeprecate==0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.3.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.21.5)\n",
            "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.10.0+cu111)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (0.13.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.7.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzmfnIsEhBC_"
      },
      "source": [
        "\n",
        "### The Model\n",
        "Let's design a 3-layer fully-connected neural network that takes as input an image that is 28x28 and outputs a probability distribution over 10 possible labels.\n",
        "\n",
        "First, let's define the model in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIxyp5ptbeF4"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class MNISTClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MNISTClassifier, self).__init__()\n",
        "\n",
        "    # mnist images are (1, 28, 28) (channels, width, height) \n",
        "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
        "    self.layer_2 = torch.nn.Linear(128, 256)\n",
        "    self.layer_3 = torch.nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, channels, width, height = x.size()\n",
        "\n",
        "    # (b, 1, 28, 28) -> (b, 1*28*28)\n",
        "    x = x.view(batch_size, -1)\n",
        "\n",
        "    # layer 1\n",
        "    x = self.layer_1(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 2\n",
        "    x = self.layer_2(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 3\n",
        "    x = self.layer_3(x)\n",
        "\n",
        "    # probability distribution over labels\n",
        "    x = torch.log_softmax(x, dim=1)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWknAwYXdTUo"
      },
      "source": [
        "That's it! This model defines the computational graph to take as input an MNIST image and convert it to a probability distribution over 10 classes for digits 0-9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAA97Rafkoc5"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class LightningMNISTClassifier(pl.LightningModule):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(LightningMNISTClassifier, self).__init__()\n",
        "\n",
        "    # mnist images are (1, 28, 28) (channels, width, height) \n",
        "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
        "    self.layer_2 = torch.nn.Linear(128, 256)\n",
        "    self.layer_3 = torch.nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, channels, width, height = x.siz()\n",
        "\n",
        "    # (b, 1, 28, 28) -> (b, 1*28*28)\n",
        "    x = x.view(batch_size, -1)\n",
        "\n",
        "    # layer 1\n",
        "    x = self.layer_1(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 2\n",
        "    x = self.layer_2(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 3\n",
        "    x = self.layer_3(x)\n",
        "\n",
        "    # probability distribution over labels\n",
        "    x = torch.log_softmax(x, dim=1)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DTyVKnNmxQR"
      },
      "source": [
        "### The Data\n",
        "Let's generate three splits of MNIST, a training, validation and test split.\n",
        "This again, is the same code in PyTorch as it is in Lightning.\n",
        "\n",
        "The dataset is added to the Dataloader which handles the loading, shuffling and batching of the dataset.\n",
        "\n",
        "In short, data preparation has 3 steps:\n",
        "1. Image transforms (these are highly subjective).\n",
        "2. Generate training, validation and test dataset splits.\n",
        "3. Wrap each dataset split in a DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNFAYG0DnF-K"
      },
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "# ----------------\n",
        "# TRANSFORMS\n",
        "# ----------------\n",
        "# prepare transforms standard to MNIST\n",
        "transform=transforms.Compose([transforms.ToTensor(), \n",
        "                              transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# ----------------\n",
        "# TRAINING, VAL DATA\n",
        "# ----------------\n",
        "mnist_train = MNIST(os.getcwd(), train=True, download=True)\n",
        "\n",
        "# train (55,000 images), val split (5,000 images)\n",
        "mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\n",
        "\n",
        "# ----------------\n",
        "# TEST DATA\n",
        "# ----------------\n",
        "mnist_test = MNIST(os.getcwd(), train=False, download=True)\n",
        "\n",
        "# ----------------\n",
        "# DATALOADERS\n",
        "# ----------------\n",
        "# The dataloaders handle shuffling, batching, etc...\n",
        "mnist_train = DataLoader(mnist_train, batch_size=64)\n",
        "mnist_val = DataLoader(mnist_val, batch_size=64)\n",
        "mnist_test = DataLoader(mnist_test, batch_size=64)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU8TXcoIpPMB"
      },
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class MNISTDataModule(pl.LightningDataModule):\n",
        "\n",
        "  def setup(self, stage):\n",
        "    # transforms for images\n",
        "    transform=transforms.Compose([transforms.ToTensor(), \n",
        "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
        "      \n",
        "    # prepare transforms standard to MNIST\n",
        "    mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n",
        "    mnist_test = MNIST(os.getcwd(), train=False, download=True, transform=transform)\n",
        "    \n",
        "    self.mnist_train, self.mnist_val = random_split(mnist_train, [55000, 5000])\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.mnist_train, batch_size=64)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.mnist_val, batch_size=64)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self,mnist_test, batch_size=64)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhQQXnT6rwyJ"
      },
      "source": [
        "pytorch_model = MNISTClassifier()\n",
        "optimizer = torch.optim.Adam(pytorch_model.parameters(), lr=1e-3)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRLagd2lsZVD"
      },
      "source": [
        "The optimizer code is the same for Lightning, except that it is added to the function ```configure_optimizers()``` in the LightningModule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juPCCQmuskIr"
      },
      "source": [
        "class LightningMNISTClassifier(pl.LightningModule):\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "      optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "      return optimizer"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DBWidT7tYt7"
      },
      "source": [
        "### The loss\n",
        "In this case, we want to take our logits and calculate the cross entropy loss. Since cross entropy is the same as NegativeLogLikelihood(log_softmax), we just need to add the nll_loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i-q_mIPtoG2"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "def cross_entropy_loss(logits, labels):\n",
        "  return F.nll_loss(logits, labels)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3y49fl3uqVH"
      },
      "source": [
        "In PyTorch Lightning we use exactly the same code for the loss. However, we could place it anywhere in the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QpX-tjku7k6"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "class LightningMNISTClassifier(pl.LightningModule):\n",
        "\n",
        "  def cross_entropy_loss(self, logits, labels):\n",
        "    return F.nll_loss(logits, labels)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U5fcnmshRrL"
      },
      "source": [
        "### PyTorch Training loop\n",
        "In PyTorch, the full training code looks like this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRRfKw5HyOWb",
        "outputId": "501b6d28-cfe7-45a3-9c71-5ac38c5e776a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "\n",
        "# -----------------\n",
        "# MODEL\n",
        "# -----------------\n",
        "class LightningMNISTClassifier(pl.LightningModule):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(LightningMNISTClassifier, self).__init__()\n",
        "\n",
        "    # mnist images are (1, 28, 28) (channels, width, height) \n",
        "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
        "    self.layer_2 = torch.nn.Linear(128, 256)\n",
        "    self.layer_3 = torch.nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, channels, width, height = x.sizes()\n",
        "\n",
        "    # (b, 1, 28, 28) -> (b, 1*28*28)\n",
        "    x = x.view(batch_size, -1)\n",
        "\n",
        "    # layer 1\n",
        "    x = self.layer_1(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 2\n",
        "    x = self.layer_2(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 3\n",
        "    x = self.layer_3(x)\n",
        "\n",
        "    # probability distribution over labels\n",
        "    x = torch.log_softmax(x, dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "# ----------------\n",
        "# DATA\n",
        "# ----------------\n",
        "transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n",
        "mnist_test = MNIST(os.getcwd(), train=False, download=True, transform=transform)\n",
        "\n",
        "# train (55,000 images), val split (5,000 images)\n",
        "mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\n",
        "mnist_test = MNIST(os.getcwd(), train=False, download=True)\n",
        "\n",
        "# The dataloaders handle shuffling, batching, etc...\n",
        "mnist_train = DataLoader(mnist_train, batch_size=64)\n",
        "mnist_val = DataLoader(mnist_val, batch_size=64)\n",
        "mnist_test = DataLoader(mnist_test, batch_size=64)\n",
        "\n",
        "# ----------------\n",
        "# OPTIMIZER\n",
        "# ----------------\n",
        "pytorch_model = MNISTClassifier()\n",
        "optimizer = torch.optim.Adam(pytorch_model.parameters(), lr=1e-3)\n",
        "\n",
        "# ----------------\n",
        "# LOSS\n",
        "# ----------------\n",
        "def cross_entropy_loss(logits, labels):\n",
        "  return F.nll_loss(logits, labels)\n",
        "\n",
        "# ----------------\n",
        "# TRAINING LOOP\n",
        "# ----------------\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  # TRAINING LOOP\n",
        "  for train_batch in mnist_train:\n",
        "    x, y = train_batch\n",
        "\n",
        "    logits = pytorch_model(x)\n",
        "    loss = cross_entropy_loss(logits, y)\n",
        "    print('train loss: ', loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  # VALIDATION LOOP\n",
        "  with torch.no_grad():\n",
        "    val_loss = []\n",
        "    for val_batch in mnist_val:\n",
        "      x, y = val_batch\n",
        "      logits = pytorch_model(x)\n",
        "      val_loss.append(cross_entropy_loss(logits, y).item())\n",
        "\n",
        "    val_loss = torch.mean(torch.tensor(val_loss))\n",
        "    print('val_loss: ', val_loss.item())\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss:  2.30081844329834\n",
            "train loss:  2.2526729106903076\n",
            "train loss:  2.2277235984802246\n",
            "train loss:  2.175123929977417\n",
            "train loss:  2.029291868209839\n",
            "train loss:  1.9520759582519531\n",
            "train loss:  1.8630157709121704\n",
            "train loss:  1.7250524759292603\n",
            "train loss:  1.5779584646224976\n",
            "train loss:  1.5145026445388794\n",
            "train loss:  1.4442952871322632\n",
            "train loss:  1.3292323350906372\n",
            "train loss:  1.234236478805542\n",
            "train loss:  1.0952024459838867\n",
            "train loss:  1.0112617015838623\n",
            "train loss:  0.9332746863365173\n",
            "train loss:  0.9097915291786194\n",
            "train loss:  0.8430405855178833\n",
            "train loss:  0.9589604139328003\n",
            "train loss:  0.9157715439796448\n",
            "train loss:  0.6909451484680176\n",
            "train loss:  0.7911491394042969\n",
            "train loss:  0.6278713345527649\n",
            "train loss:  0.8026424646377563\n",
            "train loss:  0.6991480588912964\n",
            "train loss:  0.8510854244232178\n",
            "train loss:  0.5599255561828613\n",
            "train loss:  0.54083651304245\n",
            "train loss:  0.3997839391231537\n",
            "train loss:  0.7081504464149475\n",
            "train loss:  0.6186147928237915\n",
            "train loss:  0.5871815085411072\n",
            "train loss:  0.5252087116241455\n",
            "train loss:  0.6266270875930786\n",
            "train loss:  0.47191622853279114\n",
            "train loss:  0.716742753982544\n",
            "train loss:  0.6471995711326599\n",
            "train loss:  0.43228739500045776\n",
            "train loss:  0.741438627243042\n",
            "train loss:  0.5489535927772522\n",
            "train loss:  0.6781333684921265\n",
            "train loss:  0.9196357727050781\n",
            "train loss:  0.5233331918716431\n",
            "train loss:  0.727677583694458\n",
            "train loss:  0.43559569120407104\n",
            "train loss:  0.37547776103019714\n",
            "train loss:  0.6217651963233948\n",
            "train loss:  0.8052732944488525\n",
            "train loss:  0.36385688185691833\n",
            "train loss:  0.2740747034549713\n",
            "train loss:  0.42265215516090393\n",
            "train loss:  0.38695642352104187\n",
            "train loss:  0.5592237710952759\n",
            "train loss:  0.3738974630832672\n",
            "train loss:  0.3804222345352173\n",
            "train loss:  0.5595356225967407\n",
            "train loss:  0.39787349104881287\n",
            "train loss:  0.39568042755126953\n",
            "train loss:  0.582075297832489\n",
            "train loss:  0.41160961985588074\n",
            "train loss:  0.35162708163261414\n",
            "train loss:  0.3347744643688202\n",
            "train loss:  0.49132898449897766\n",
            "train loss:  0.41459164023399353\n",
            "train loss:  0.4224143326282501\n",
            "train loss:  0.30075445771217346\n",
            "train loss:  0.398309588432312\n",
            "train loss:  0.2795953154563904\n",
            "train loss:  0.47967222332954407\n",
            "train loss:  0.3315640687942505\n",
            "train loss:  0.3652604818344116\n",
            "train loss:  0.3272460103034973\n",
            "train loss:  0.36046868562698364\n",
            "train loss:  0.4238659739494324\n",
            "train loss:  0.28747954964637756\n",
            "train loss:  0.23695647716522217\n",
            "train loss:  0.38517501950263977\n",
            "train loss:  0.46048879623413086\n",
            "train loss:  0.43754148483276367\n",
            "train loss:  0.4539434015750885\n",
            "train loss:  0.2545228898525238\n",
            "train loss:  0.3116835355758667\n",
            "train loss:  0.6045506596565247\n",
            "train loss:  0.3363579511642456\n",
            "train loss:  0.31294411420822144\n",
            "train loss:  0.5297679305076599\n",
            "train loss:  0.29332873225212097\n",
            "train loss:  0.31693288683891296\n",
            "train loss:  0.5310574173927307\n",
            "train loss:  0.4009566307067871\n",
            "train loss:  0.32031285762786865\n",
            "train loss:  0.3872268795967102\n",
            "train loss:  0.17513108253479004\n",
            "train loss:  0.5520628094673157\n",
            "train loss:  0.20157290995121002\n",
            "train loss:  0.4154844284057617\n",
            "train loss:  0.23680473864078522\n",
            "train loss:  0.38732489943504333\n",
            "train loss:  0.6132734417915344\n",
            "train loss:  0.27693018317222595\n",
            "train loss:  0.3024403154850006\n",
            "train loss:  0.25677576661109924\n",
            "train loss:  0.49063029885292053\n",
            "train loss:  0.2913872003555298\n",
            "train loss:  0.22414711117744446\n",
            "train loss:  0.5107289552688599\n",
            "train loss:  0.33268657326698303\n",
            "train loss:  0.3110813796520233\n",
            "train loss:  0.2745554447174072\n",
            "train loss:  0.4824362099170685\n",
            "train loss:  0.30566275119781494\n",
            "train loss:  0.6499820351600647\n",
            "train loss:  0.26910197734832764\n",
            "train loss:  0.2780800461769104\n",
            "train loss:  0.5117249488830566\n",
            "train loss:  0.4233657121658325\n",
            "train loss:  0.30530112981796265\n",
            "train loss:  0.4897162914276123\n",
            "train loss:  0.23166142404079437\n",
            "train loss:  0.28435343503952026\n",
            "train loss:  0.3612626791000366\n",
            "train loss:  0.6237984895706177\n",
            "train loss:  0.19991913437843323\n",
            "train loss:  0.42315474152565\n",
            "train loss:  0.36149099469184875\n",
            "train loss:  0.30592381954193115\n",
            "train loss:  0.24984245002269745\n",
            "train loss:  0.18709029257297516\n",
            "train loss:  0.5022794008255005\n",
            "train loss:  0.1814091056585312\n",
            "train loss:  0.24914325773715973\n",
            "train loss:  0.21183601021766663\n",
            "train loss:  0.4066506624221802\n",
            "train loss:  0.31350359320640564\n",
            "train loss:  0.30414456129074097\n",
            "train loss:  0.2331051081418991\n",
            "train loss:  0.2142927050590515\n",
            "train loss:  0.22155043482780457\n",
            "train loss:  0.4536590874195099\n",
            "train loss:  0.34277477860450745\n",
            "train loss:  0.14024411141872406\n",
            "train loss:  0.16581735014915466\n",
            "train loss:  0.24447406828403473\n",
            "train loss:  0.3657008409500122\n",
            "train loss:  0.28887566924095154\n",
            "train loss:  0.2989683151245117\n",
            "train loss:  0.5692293643951416\n",
            "train loss:  0.29955676198005676\n",
            "train loss:  0.41508740186691284\n",
            "train loss:  0.3275792598724365\n",
            "train loss:  0.32084277272224426\n",
            "train loss:  0.24934208393096924\n",
            "train loss:  0.3310070037841797\n",
            "train loss:  0.5421818494796753\n",
            "train loss:  0.38317131996154785\n",
            "train loss:  0.14125986397266388\n",
            "train loss:  0.221303328871727\n",
            "train loss:  0.13628016412258148\n",
            "train loss:  0.17381823062896729\n",
            "train loss:  0.3354189097881317\n",
            "train loss:  0.15171295404434204\n",
            "train loss:  0.2466048300266266\n",
            "train loss:  0.1996467560529709\n",
            "train loss:  0.4272604286670685\n",
            "train loss:  0.08909677714109421\n",
            "train loss:  0.2957550585269928\n",
            "train loss:  0.3127593398094177\n",
            "train loss:  0.41636040806770325\n",
            "train loss:  0.15765653550624847\n",
            "train loss:  0.1600302904844284\n",
            "train loss:  0.48474693298339844\n",
            "train loss:  0.3897645175457001\n",
            "train loss:  0.12169890105724335\n",
            "train loss:  0.3177769184112549\n",
            "train loss:  0.269205778837204\n",
            "train loss:  0.24211888015270233\n",
            "train loss:  0.27402037382125854\n",
            "train loss:  0.19291718304157257\n",
            "train loss:  0.1165451928973198\n",
            "train loss:  0.4173060953617096\n",
            "train loss:  0.15940719842910767\n",
            "train loss:  0.4130585789680481\n",
            "train loss:  0.39203184843063354\n",
            "train loss:  0.38408830761909485\n",
            "train loss:  0.38483062386512756\n",
            "train loss:  0.30095797777175903\n",
            "train loss:  0.4006563723087311\n",
            "train loss:  0.3198355436325073\n",
            "train loss:  0.2190723419189453\n",
            "train loss:  0.21264994144439697\n",
            "train loss:  0.40388908982276917\n",
            "train loss:  0.3640110194683075\n",
            "train loss:  0.38031205534935\n",
            "train loss:  0.22227846086025238\n",
            "train loss:  0.21229377388954163\n",
            "train loss:  0.2690511643886566\n",
            "train loss:  0.4037444591522217\n",
            "train loss:  0.29378876090049744\n",
            "train loss:  0.3304935097694397\n",
            "train loss:  0.19192492961883545\n",
            "train loss:  0.5838561058044434\n",
            "train loss:  0.34216398000717163\n",
            "train loss:  0.3282109498977661\n",
            "train loss:  0.17451754212379456\n",
            "train loss:  0.21383897960186005\n",
            "train loss:  0.18287958204746246\n",
            "train loss:  0.4117322266101837\n",
            "train loss:  0.2639254629611969\n",
            "train loss:  0.2812471091747284\n",
            "train loss:  0.2779053747653961\n",
            "train loss:  0.252782940864563\n",
            "train loss:  0.2628966271877289\n",
            "train loss:  0.12754744291305542\n",
            "train loss:  0.14632351696491241\n",
            "train loss:  0.5007753372192383\n",
            "train loss:  0.24355918169021606\n",
            "train loss:  0.4281928837299347\n",
            "train loss:  0.15853501856327057\n",
            "train loss:  0.12940557301044464\n",
            "train loss:  0.0982886403799057\n",
            "train loss:  0.3066018521785736\n",
            "train loss:  0.17680010199546814\n",
            "train loss:  0.19808101654052734\n",
            "train loss:  0.3092651665210724\n",
            "train loss:  0.4295613765716553\n",
            "train loss:  0.1285472810268402\n",
            "train loss:  0.07395660132169724\n",
            "train loss:  0.1797151118516922\n",
            "train loss:  0.27196192741394043\n",
            "train loss:  0.1829833686351776\n",
            "train loss:  0.21729479730129242\n",
            "train loss:  0.21778243780136108\n",
            "train loss:  0.18870793282985687\n",
            "train loss:  0.41432884335517883\n",
            "train loss:  0.15944543480873108\n",
            "train loss:  0.12865538895130157\n",
            "train loss:  0.1117519661784172\n",
            "train loss:  0.2378278374671936\n",
            "train loss:  0.1666267365217209\n",
            "train loss:  0.20100386440753937\n",
            "train loss:  0.21511633694171906\n",
            "train loss:  0.3158295750617981\n",
            "train loss:  0.18339146673679352\n",
            "train loss:  0.2208549827337265\n",
            "train loss:  0.1534842550754547\n",
            "train loss:  0.26896902918815613\n",
            "train loss:  0.2601945102214813\n",
            "train loss:  0.2724013328552246\n",
            "train loss:  0.1708623170852661\n",
            "train loss:  0.14760299026966095\n",
            "train loss:  0.11915117502212524\n",
            "train loss:  0.27036747336387634\n",
            "train loss:  0.493030309677124\n",
            "train loss:  0.0990324541926384\n",
            "train loss:  0.22421777248382568\n",
            "train loss:  0.14867067337036133\n",
            "train loss:  0.08521690219640732\n",
            "train loss:  0.20386551320552826\n",
            "train loss:  0.23872391879558563\n",
            "train loss:  0.2449144423007965\n",
            "train loss:  0.30409812927246094\n",
            "train loss:  0.16177132725715637\n",
            "train loss:  0.28902411460876465\n",
            "train loss:  0.13798180222511292\n",
            "train loss:  0.08894757926464081\n",
            "train loss:  0.16609510779380798\n",
            "train loss:  0.23903724551200867\n",
            "train loss:  0.4535219371318817\n",
            "train loss:  0.2417992204427719\n",
            "train loss:  0.09755519777536392\n",
            "train loss:  0.23782899975776672\n",
            "train loss:  0.4493153989315033\n",
            "train loss:  0.1548772156238556\n",
            "train loss:  0.23630130290985107\n",
            "train loss:  0.3406192660331726\n",
            "train loss:  0.37267985939979553\n",
            "train loss:  0.14463338255882263\n",
            "train loss:  0.40624406933784485\n",
            "train loss:  0.2885441184043884\n",
            "train loss:  0.34591710567474365\n",
            "train loss:  0.34251269698143005\n",
            "train loss:  0.47697046399116516\n",
            "train loss:  0.09478115290403366\n",
            "train loss:  0.15311864018440247\n",
            "train loss:  0.08296571671962738\n",
            "train loss:  0.309560090303421\n",
            "train loss:  0.24507835507392883\n",
            "train loss:  0.17124035954475403\n",
            "train loss:  0.19737331569194794\n",
            "train loss:  0.18875999748706818\n",
            "train loss:  0.10117770731449127\n",
            "train loss:  0.25626328587532043\n",
            "train loss:  0.2786312699317932\n",
            "train loss:  0.4084698259830475\n",
            "train loss:  0.11025509238243103\n",
            "train loss:  0.1396035999059677\n",
            "train loss:  0.41941556334495544\n",
            "train loss:  0.20632219314575195\n",
            "train loss:  0.28368237614631653\n",
            "train loss:  0.11676900833845139\n",
            "train loss:  0.2584642767906189\n",
            "train loss:  0.20958060026168823\n",
            "train loss:  0.14069880545139313\n",
            "train loss:  0.18437622487545013\n",
            "train loss:  0.31306013464927673\n",
            "train loss:  0.1464080959558487\n",
            "train loss:  0.25743386149406433\n",
            "train loss:  0.23719751834869385\n",
            "train loss:  0.2997856140136719\n",
            "train loss:  0.23083853721618652\n",
            "train loss:  0.3315102458000183\n",
            "train loss:  0.3326784372329712\n",
            "train loss:  0.2977895140647888\n",
            "train loss:  0.2628706097602844\n",
            "train loss:  0.1660747230052948\n",
            "train loss:  0.30428847670555115\n",
            "train loss:  0.29921162128448486\n",
            "train loss:  0.19293271005153656\n",
            "train loss:  0.16289962828159332\n",
            "train loss:  0.16291704773902893\n",
            "train loss:  0.3724636733531952\n",
            "train loss:  0.18363626301288605\n",
            "train loss:  0.2195117026567459\n",
            "train loss:  0.2581009864807129\n",
            "train loss:  0.2403518259525299\n",
            "train loss:  0.18361391127109528\n",
            "train loss:  0.2339385598897934\n",
            "train loss:  0.33897534012794495\n",
            "train loss:  0.1844676434993744\n",
            "train loss:  0.2951398491859436\n",
            "train loss:  0.10455632954835892\n",
            "train loss:  0.11602663993835449\n",
            "train loss:  0.35411182045936584\n",
            "train loss:  0.19975987076759338\n",
            "train loss:  0.1856183111667633\n",
            "train loss:  0.15933774411678314\n",
            "train loss:  0.3742535710334778\n",
            "train loss:  0.14188872277736664\n",
            "train loss:  0.33001911640167236\n",
            "train loss:  0.2524479031562805\n",
            "train loss:  0.35478734970092773\n",
            "train loss:  0.17728191614151\n",
            "train loss:  0.19257867336273193\n",
            "train loss:  0.23815545439720154\n",
            "train loss:  0.1914660930633545\n",
            "train loss:  0.13897614181041718\n",
            "train loss:  0.2202529013156891\n",
            "train loss:  0.3050674796104431\n",
            "train loss:  0.21515372395515442\n",
            "train loss:  0.2490561306476593\n",
            "train loss:  0.1648331731557846\n",
            "train loss:  0.34983235597610474\n",
            "train loss:  0.07687845081090927\n",
            "train loss:  0.28250181674957275\n",
            "train loss:  0.15914879739284515\n",
            "train loss:  0.14573456346988678\n",
            "train loss:  0.13775750994682312\n",
            "train loss:  0.12490919232368469\n",
            "train loss:  0.11794497817754745\n",
            "train loss:  0.10182145237922668\n",
            "train loss:  0.214480921626091\n",
            "train loss:  0.10433182120323181\n",
            "train loss:  0.24011169373989105\n",
            "train loss:  0.14241619408130646\n",
            "train loss:  0.19481544196605682\n",
            "train loss:  0.09631382673978806\n",
            "train loss:  0.16616922616958618\n",
            "train loss:  0.175070658326149\n",
            "train loss:  0.38049253821372986\n",
            "train loss:  0.06527157127857208\n",
            "train loss:  0.16568319499492645\n",
            "train loss:  0.1501774936914444\n",
            "train loss:  0.29018768668174744\n",
            "train loss:  0.15049244463443756\n",
            "train loss:  0.3277089297771454\n",
            "train loss:  0.22069789469242096\n",
            "train loss:  0.11007869988679886\n",
            "train loss:  0.16459119319915771\n",
            "train loss:  0.07775654643774033\n",
            "train loss:  0.14521309733390808\n",
            "train loss:  0.2288637012243271\n",
            "train loss:  0.19428154826164246\n",
            "train loss:  0.22913232445716858\n",
            "train loss:  0.11604674160480499\n",
            "train loss:  0.21698139607906342\n",
            "train loss:  0.29871824383735657\n",
            "train loss:  0.07528465986251831\n",
            "train loss:  0.39362117648124695\n",
            "train loss:  0.18355993926525116\n",
            "train loss:  0.2478792369365692\n",
            "train loss:  0.18263399600982666\n",
            "train loss:  0.1228470727801323\n",
            "train loss:  0.17471559345722198\n",
            "train loss:  0.279655784368515\n",
            "train loss:  0.10431558638811111\n",
            "train loss:  0.45566269755363464\n",
            "train loss:  0.3278657793998718\n",
            "train loss:  0.0770408883690834\n",
            "train loss:  0.13925839960575104\n",
            "train loss:  0.2057076096534729\n",
            "train loss:  0.2132936418056488\n",
            "train loss:  0.13751696050167084\n",
            "train loss:  0.07241533696651459\n",
            "train loss:  0.24846576154232025\n",
            "train loss:  0.1119721308350563\n",
            "train loss:  0.1601632684469223\n",
            "train loss:  0.21643370389938354\n",
            "train loss:  0.07671374082565308\n",
            "train loss:  0.16336245834827423\n",
            "train loss:  0.2081833779811859\n",
            "train loss:  0.07750929892063141\n",
            "train loss:  0.1881338655948639\n",
            "train loss:  0.4000776708126068\n",
            "train loss:  0.16356714069843292\n",
            "train loss:  0.14620356261730194\n",
            "train loss:  0.29005569219589233\n",
            "train loss:  0.24867859482765198\n",
            "train loss:  0.14445416629314423\n",
            "train loss:  0.13021421432495117\n",
            "train loss:  0.34906819462776184\n",
            "train loss:  0.3249037563800812\n",
            "train loss:  0.09023220837116241\n",
            "train loss:  0.27053675055503845\n",
            "train loss:  0.389864444732666\n",
            "train loss:  0.3878870904445648\n",
            "train loss:  0.1341019719839096\n",
            "train loss:  0.36245253682136536\n",
            "train loss:  0.12890903651714325\n",
            "train loss:  0.12381498515605927\n",
            "train loss:  0.1353168487548828\n",
            "train loss:  0.24087940156459808\n",
            "train loss:  0.22353120148181915\n",
            "train loss:  0.18409961462020874\n",
            "train loss:  0.2278682142496109\n",
            "train loss:  0.08202298730611801\n",
            "train loss:  0.18674784898757935\n",
            "train loss:  0.18210065364837646\n",
            "train loss:  0.04916480928659439\n",
            "train loss:  0.11585062742233276\n",
            "train loss:  0.18295733630657196\n",
            "train loss:  0.16250190138816833\n",
            "train loss:  0.12189263105392456\n",
            "train loss:  0.11768811196088791\n",
            "train loss:  0.12581561505794525\n",
            "train loss:  0.29110831022262573\n",
            "train loss:  0.17948085069656372\n",
            "train loss:  0.0824863612651825\n",
            "train loss:  0.20115092396736145\n",
            "train loss:  0.2192809283733368\n",
            "train loss:  0.31833386421203613\n",
            "train loss:  0.06858766078948975\n",
            "train loss:  0.24147072434425354\n",
            "train loss:  0.15037831664085388\n",
            "train loss:  0.11436979472637177\n",
            "train loss:  0.2230563759803772\n",
            "train loss:  0.28995436429977417\n",
            "train loss:  0.16212132573127747\n",
            "train loss:  0.23971588909626007\n",
            "train loss:  0.09909635782241821\n",
            "train loss:  0.23674237728118896\n",
            "train loss:  0.1954178512096405\n",
            "train loss:  0.19813202321529388\n",
            "train loss:  0.13387607038021088\n",
            "train loss:  0.1546691656112671\n",
            "train loss:  0.06506286561489105\n",
            "train loss:  0.16013777256011963\n",
            "train loss:  0.19256332516670227\n",
            "train loss:  0.19236072897911072\n",
            "train loss:  0.14737625420093536\n",
            "train loss:  0.19716139137744904\n",
            "train loss:  0.06197827309370041\n",
            "train loss:  0.12801407277584076\n",
            "train loss:  0.16638055443763733\n",
            "train loss:  0.06000477075576782\n",
            "train loss:  0.153122216463089\n",
            "train loss:  0.06234782934188843\n",
            "train loss:  0.1540726125240326\n",
            "train loss:  0.12114788591861725\n",
            "train loss:  0.17517271637916565\n",
            "train loss:  0.166582390666008\n",
            "train loss:  0.142664834856987\n",
            "train loss:  0.22333523631095886\n",
            "train loss:  0.09244511276483536\n",
            "train loss:  0.2328779101371765\n",
            "train loss:  0.3885217308998108\n",
            "train loss:  0.3517495393753052\n",
            "train loss:  0.08538613468408585\n",
            "train loss:  0.39743873476982117\n",
            "train loss:  0.23801986873149872\n",
            "train loss:  0.0730304941534996\n",
            "train loss:  0.11889686435461044\n",
            "train loss:  0.3141575753688812\n",
            "train loss:  0.10616330057382584\n",
            "train loss:  0.04517485946416855\n",
            "train loss:  0.22090011835098267\n",
            "train loss:  0.3070838153362274\n",
            "train loss:  0.15747053921222687\n",
            "train loss:  0.1683533787727356\n",
            "train loss:  0.18079529702663422\n",
            "train loss:  0.10635937005281448\n",
            "train loss:  0.3005825877189636\n",
            "train loss:  0.17516770958900452\n",
            "train loss:  0.08991482853889465\n",
            "train loss:  0.1540961116552353\n",
            "train loss:  0.28363358974456787\n",
            "train loss:  0.12711431086063385\n",
            "train loss:  0.1587495356798172\n",
            "train loss:  0.1164504811167717\n",
            "train loss:  0.2694730758666992\n",
            "train loss:  0.0953601524233818\n",
            "train loss:  0.1618868112564087\n",
            "train loss:  0.39900174736976624\n",
            "train loss:  0.15893907845020294\n",
            "train loss:  0.11530689150094986\n",
            "train loss:  0.2279001623392105\n",
            "train loss:  0.05333742871880531\n",
            "train loss:  0.3011510968208313\n",
            "train loss:  0.05584138631820679\n",
            "train loss:  0.21664384007453918\n",
            "train loss:  0.24585509300231934\n",
            "train loss:  0.19828547537326813\n",
            "train loss:  0.10192738473415375\n",
            "train loss:  0.07866429537534714\n",
            "train loss:  0.18124766647815704\n",
            "train loss:  0.25215357542037964\n",
            "train loss:  0.07020293176174164\n",
            "train loss:  0.1820521056652069\n",
            "train loss:  0.16287261247634888\n",
            "train loss:  0.10659947246313095\n",
            "train loss:  0.12545497715473175\n",
            "train loss:  0.28872403502464294\n",
            "train loss:  0.04556858912110329\n",
            "train loss:  0.08173113316297531\n",
            "train loss:  0.047147832810878754\n",
            "train loss:  0.14304496347904205\n",
            "train loss:  0.079894058406353\n",
            "train loss:  0.1446773260831833\n",
            "train loss:  0.13542859256267548\n",
            "train loss:  0.21674185991287231\n",
            "train loss:  0.12237684428691864\n",
            "train loss:  0.21282166242599487\n",
            "train loss:  0.4828069806098938\n",
            "train loss:  0.09234680980443954\n",
            "train loss:  0.14225034415721893\n",
            "train loss:  0.05806313082575798\n",
            "train loss:  0.09455496072769165\n",
            "train loss:  0.06540713459253311\n",
            "train loss:  0.09221435338258743\n",
            "train loss:  0.04968647658824921\n",
            "train loss:  0.11835063993930817\n",
            "train loss:  0.18923349678516388\n",
            "train loss:  0.1754998415708542\n",
            "train loss:  0.2564745247364044\n",
            "train loss:  0.19021518528461456\n",
            "train loss:  0.08884827047586441\n",
            "train loss:  0.2486024796962738\n",
            "train loss:  0.1618993878364563\n",
            "train loss:  0.4273504912853241\n",
            "train loss:  0.3236287534236908\n",
            "train loss:  0.16227631270885468\n",
            "train loss:  0.054923586547374725\n",
            "train loss:  0.2181246131658554\n",
            "train loss:  0.07715559005737305\n",
            "train loss:  0.09188193827867508\n",
            "train loss:  0.17692436277866364\n",
            "train loss:  0.22754091024398804\n",
            "train loss:  0.18057283759117126\n",
            "train loss:  0.06075996905565262\n",
            "train loss:  0.07174351811408997\n",
            "train loss:  0.2081771045923233\n",
            "train loss:  0.13658283650875092\n",
            "train loss:  0.12761512398719788\n",
            "train loss:  0.223415344953537\n",
            "train loss:  0.21877695620059967\n",
            "train loss:  0.10528795421123505\n",
            "train loss:  0.1599583625793457\n",
            "train loss:  0.11861763894557953\n",
            "train loss:  0.10909701138734818\n",
            "train loss:  0.2588973939418793\n",
            "train loss:  0.09437378495931625\n",
            "train loss:  0.2175866961479187\n",
            "train loss:  0.252480149269104\n",
            "train loss:  0.0867571160197258\n",
            "train loss:  0.15980923175811768\n",
            "train loss:  0.24598972499370575\n",
            "train loss:  0.19600878655910492\n",
            "train loss:  0.22423338890075684\n",
            "train loss:  0.28677916526794434\n",
            "train loss:  0.1516255885362625\n",
            "train loss:  0.1513475626707077\n",
            "train loss:  0.05166691541671753\n",
            "train loss:  0.10989024490118027\n",
            "train loss:  0.11840514838695526\n",
            "train loss:  0.20257161557674408\n",
            "train loss:  0.04540891945362091\n",
            "train loss:  0.1336214542388916\n",
            "train loss:  0.23653316497802734\n",
            "train loss:  0.13059686124324799\n",
            "train loss:  0.12111411988735199\n",
            "train loss:  0.12873046100139618\n",
            "train loss:  0.12239211052656174\n",
            "train loss:  0.28415700793266296\n",
            "train loss:  0.09975682944059372\n",
            "train loss:  0.37588033080101013\n",
            "train loss:  0.20753678679466248\n",
            "train loss:  0.1510762721300125\n",
            "train loss:  0.1119130551815033\n",
            "train loss:  0.0809084102511406\n",
            "train loss:  0.12841777503490448\n",
            "train loss:  0.10662408918142319\n",
            "train loss:  0.26018577814102173\n",
            "train loss:  0.1640988290309906\n",
            "train loss:  0.03769722580909729\n",
            "train loss:  0.10919412970542908\n",
            "train loss:  0.25487786531448364\n",
            "train loss:  0.17812232673168182\n",
            "train loss:  0.2688360810279846\n",
            "train loss:  0.14860811829566956\n",
            "train loss:  0.13879455626010895\n",
            "train loss:  0.11868486553430557\n",
            "train loss:  0.2633945047855377\n",
            "train loss:  0.21439321339130402\n",
            "train loss:  0.06850536912679672\n",
            "train loss:  0.29048165678977966\n",
            "train loss:  0.12510718405246735\n",
            "train loss:  0.11946559697389603\n",
            "train loss:  0.22593918442726135\n",
            "train loss:  0.0805516242980957\n",
            "train loss:  0.22451333701610565\n",
            "train loss:  0.13981452584266663\n",
            "train loss:  0.24598172307014465\n",
            "train loss:  0.19601836800575256\n",
            "train loss:  0.02954326756298542\n",
            "train loss:  0.14957867562770844\n",
            "train loss:  0.06730131059885025\n",
            "train loss:  0.18088644742965698\n",
            "train loss:  0.1056189015507698\n",
            "train loss:  0.14608179032802582\n",
            "train loss:  0.299703985452652\n",
            "train loss:  0.15087543427944183\n",
            "train loss:  0.178551584482193\n",
            "train loss:  0.2098248302936554\n",
            "train loss:  0.16466094553470612\n",
            "train loss:  0.19895699620246887\n",
            "train loss:  0.20947572588920593\n",
            "train loss:  0.16201703250408173\n",
            "train loss:  0.061441726982593536\n",
            "train loss:  0.11023949831724167\n",
            "train loss:  0.09472425282001495\n",
            "train loss:  0.2856922745704651\n",
            "train loss:  0.14308708906173706\n",
            "train loss:  0.12317577004432678\n",
            "train loss:  0.10520274192094803\n",
            "train loss:  0.11922512203454971\n",
            "train loss:  0.1460481435060501\n",
            "train loss:  0.1583489626646042\n",
            "train loss:  0.24605748057365417\n",
            "train loss:  0.1543361097574234\n",
            "train loss:  0.1437433958053589\n",
            "train loss:  0.260429322719574\n",
            "train loss:  0.14862817525863647\n",
            "train loss:  0.340415358543396\n",
            "train loss:  0.40299156308174133\n",
            "train loss:  0.12760475277900696\n",
            "train loss:  0.09519807249307632\n",
            "train loss:  0.1688488870859146\n",
            "train loss:  0.1094907596707344\n",
            "train loss:  0.3295039236545563\n",
            "train loss:  0.10968088358640671\n",
            "train loss:  0.20319169759750366\n",
            "train loss:  0.06280683726072311\n",
            "train loss:  0.0939602330327034\n",
            "train loss:  0.1430254578590393\n",
            "train loss:  0.09199335426092148\n",
            "train loss:  0.10266967862844467\n",
            "train loss:  0.14455777406692505\n",
            "train loss:  0.278064489364624\n",
            "train loss:  0.13123227655887604\n",
            "train loss:  0.05571337416768074\n",
            "train loss:  0.22592061758041382\n",
            "train loss:  0.03698611259460449\n",
            "train loss:  0.14245498180389404\n",
            "train loss:  0.17119289934635162\n",
            "train loss:  0.08848642557859421\n",
            "train loss:  0.12538708746433258\n",
            "train loss:  0.09922586381435394\n",
            "train loss:  0.13470450043678284\n",
            "train loss:  0.118356853723526\n",
            "train loss:  0.13418763875961304\n",
            "train loss:  0.10518790036439896\n",
            "train loss:  0.11901163309812546\n",
            "train loss:  0.11012017726898193\n",
            "train loss:  0.24337373673915863\n",
            "train loss:  0.09775935113430023\n",
            "train loss:  0.13506072759628296\n",
            "train loss:  0.04797857254743576\n",
            "train loss:  0.10325942933559418\n",
            "train loss:  0.10596264153718948\n",
            "train loss:  0.19709384441375732\n",
            "train loss:  0.2475632280111313\n",
            "train loss:  0.3263583481311798\n",
            "train loss:  0.12572476267814636\n",
            "train loss:  0.053010519593954086\n",
            "train loss:  0.12761983275413513\n",
            "train loss:  0.14679960906505585\n",
            "train loss:  0.19506196677684784\n",
            "train loss:  0.1427910327911377\n",
            "train loss:  0.14043046534061432\n",
            "train loss:  0.058546364307403564\n",
            "train loss:  0.24650651216506958\n",
            "train loss:  0.2624838650226593\n",
            "train loss:  0.31193676590919495\n",
            "train loss:  0.14362284541130066\n",
            "train loss:  0.1571611762046814\n",
            "train loss:  0.22564201056957245\n",
            "train loss:  0.06611821055412292\n",
            "train loss:  0.24704015254974365\n",
            "train loss:  0.07706001400947571\n",
            "train loss:  0.11045201122760773\n",
            "train loss:  0.13781610131263733\n",
            "train loss:  0.355027437210083\n",
            "train loss:  0.11395051330327988\n",
            "train loss:  0.16382743418216705\n",
            "train loss:  0.12653104960918427\n",
            "train loss:  0.16757149994373322\n",
            "train loss:  0.20670516788959503\n",
            "train loss:  0.172856405377388\n",
            "train loss:  0.284108966588974\n",
            "train loss:  0.06946026533842087\n",
            "train loss:  0.1777844876050949\n",
            "train loss:  0.12164811789989471\n",
            "train loss:  0.13947808742523193\n",
            "train loss:  0.29586172103881836\n",
            "train loss:  0.12288752943277359\n",
            "train loss:  0.19183649122714996\n",
            "train loss:  0.09916643053293228\n",
            "train loss:  0.24170321226119995\n",
            "train loss:  0.20190976560115814\n",
            "train loss:  0.11100602149963379\n",
            "train loss:  0.19189785420894623\n",
            "train loss:  0.24222610890865326\n",
            "train loss:  0.257558673620224\n",
            "train loss:  0.15317344665527344\n",
            "train loss:  0.1533040553331375\n",
            "train loss:  0.15282675623893738\n",
            "train loss:  0.19270306825637817\n",
            "train loss:  0.25214770436286926\n",
            "train loss:  0.1054479330778122\n",
            "train loss:  0.16290095448493958\n",
            "train loss:  0.1913401335477829\n",
            "train loss:  0.051495496183633804\n",
            "train loss:  0.3083110451698303\n",
            "train loss:  0.15122076869010925\n",
            "train loss:  0.23131819069385529\n",
            "train loss:  0.27164226770401\n",
            "train loss:  0.07228315621614456\n",
            "train loss:  0.22986623644828796\n",
            "train loss:  0.14153103530406952\n",
            "train loss:  0.24482420086860657\n",
            "train loss:  0.11631161719560623\n",
            "train loss:  0.0731792077422142\n",
            "train loss:  0.027516262605786324\n",
            "train loss:  0.21857240796089172\n",
            "train loss:  0.10842341929674149\n",
            "train loss:  0.1067204549908638\n",
            "train loss:  0.16214396059513092\n",
            "train loss:  0.06426703929901123\n",
            "train loss:  0.2796396017074585\n",
            "train loss:  0.24410848319530487\n",
            "train loss:  0.13878825306892395\n",
            "train loss:  0.09110236167907715\n",
            "train loss:  0.16581369936466217\n",
            "train loss:  0.14562322199344635\n",
            "train loss:  0.14435775578022003\n",
            "train loss:  0.15555770695209503\n",
            "train loss:  0.20460505783557892\n",
            "train loss:  0.10782507807016373\n",
            "train loss:  0.09056077897548676\n",
            "train loss:  0.1671050786972046\n",
            "train loss:  0.07753589749336243\n",
            "train loss:  0.14061258733272552\n",
            "train loss:  0.06770388782024384\n",
            "train loss:  0.21713244915008545\n",
            "train loss:  0.11633624881505966\n",
            "train loss:  0.13298964500427246\n",
            "train loss:  0.14468613266944885\n",
            "train loss:  0.16804400086402893\n",
            "train loss:  0.09136119484901428\n",
            "train loss:  0.23250837624073029\n",
            "train loss:  0.11306793987751007\n",
            "train loss:  0.20221047103405\n",
            "train loss:  0.10533937066793442\n",
            "train loss:  0.2577449679374695\n",
            "train loss:  0.08230679482221603\n",
            "train loss:  0.14331147074699402\n",
            "train loss:  0.33228927850723267\n",
            "train loss:  0.11458656936883926\n",
            "train loss:  0.2088247537612915\n",
            "train loss:  0.1372688114643097\n",
            "train loss:  0.16519440710544586\n",
            "train loss:  0.14694543182849884\n",
            "train loss:  0.13186560571193695\n",
            "train loss:  0.17670439183712006\n",
            "train loss:  0.08309365063905716\n",
            "train loss:  0.22451385855674744\n",
            "train loss:  0.20174121856689453\n",
            "train loss:  0.14467786252498627\n",
            "train loss:  0.15701600909233093\n",
            "train loss:  0.2112015336751938\n",
            "train loss:  0.10598403960466385\n",
            "train loss:  0.14815950393676758\n",
            "train loss:  0.298878937959671\n",
            "train loss:  0.1905371993780136\n",
            "train loss:  0.06480351835489273\n",
            "train loss:  0.13240954279899597\n",
            "train loss:  0.15755672752857208\n",
            "train loss:  0.07173173874616623\n",
            "train loss:  0.02713175304234028\n",
            "train loss:  0.040181614458560944\n",
            "train loss:  0.11716730147600174\n",
            "train loss:  0.29035240411758423\n",
            "train loss:  0.10929203033447266\n",
            "train loss:  0.07976516336202621\n",
            "train loss:  0.4261801540851593\n",
            "train loss:  0.1679428368806839\n",
            "train loss:  0.14721153676509857\n",
            "train loss:  0.09209508448839188\n",
            "train loss:  0.24510909616947174\n",
            "train loss:  0.12125993520021439\n",
            "train loss:  0.17099808156490326\n",
            "train loss:  0.22975711524486542\n",
            "train loss:  0.1197764202952385\n",
            "train loss:  0.10456697642803192\n",
            "train loss:  0.19235044717788696\n",
            "train loss:  0.15716451406478882\n",
            "train loss:  0.16200631856918335\n",
            "train loss:  0.12409192323684692\n",
            "train loss:  0.28783026337623596\n",
            "train loss:  0.0867113396525383\n",
            "train loss:  0.06676517426967621\n",
            "train loss:  0.0797150656580925\n",
            "train loss:  0.1067340299487114\n",
            "train loss:  0.17432565987110138\n",
            "train loss:  0.2742135524749756\n",
            "train loss:  0.19737659394741058\n",
            "train loss:  0.08437924832105637\n",
            "train loss:  0.10484001785516739\n",
            "train loss:  0.21388249099254608\n",
            "train loss:  0.1422179788351059\n",
            "train loss:  0.0644223541021347\n",
            "train loss:  0.17684102058410645\n",
            "train loss:  0.23986245691776276\n",
            "train loss:  0.08407839387655258\n",
            "train loss:  0.10857953876256943\n",
            "train loss:  0.08425211161375046\n",
            "train loss:  0.13599231839179993\n",
            "train loss:  0.2171589583158493\n",
            "train loss:  0.23997032642364502\n",
            "train loss:  0.1354800909757614\n",
            "train loss:  0.1344563513994217\n",
            "val_loss:  0.14042814075946808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLakGfN_ymuo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}