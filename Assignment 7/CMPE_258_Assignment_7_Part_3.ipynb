{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajviishah/CMPE-258/blob/main/Assignment%207/CMPE_258_Assignment_7_Part_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3600ee25c8e"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71c626bbac35"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4041a2e9b310"
      },
      "source": [
        "## Built-in RNN layers: a simple example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5617759e54e"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential()\n",
        "# Add an Embedding layer expecting input vocab of size 1000, and\n",
        "# output embedding dimension of size 64.\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "\n",
        "# Add a LSTM layer with 128 internal units.\n",
        "model.add(layers.LSTM(128))\n",
        "\n",
        "# Add a Dense layer with 10 units.\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3294dec91e4"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential()\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "\n",
        "# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n",
        "model.add(layers.GRU(256, return_sequences=True))\n",
        "\n",
        "# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n",
        "model.add(layers.SimpleRNN(128))\n",
        "\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ece412e6afbe"
      },
      "outputs": [],
      "source": [
        "encoder_vocab = 1000\n",
        "decoder_vocab = 2000\n",
        "\n",
        "encoder_input = layers.Input(shape=(None,))\n",
        "encoder_embedded = layers.Embedding(input_dim=encoder_vocab, output_dim=64)(\n",
        "    encoder_input\n",
        ")\n",
        "\n",
        "# Return states in addition to output\n",
        "output, state_h, state_c = layers.LSTM(64, return_state=True, name=\"encoder\")(\n",
        "    encoder_embedded\n",
        ")\n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "decoder_input = layers.Input(shape=(None,))\n",
        "decoder_embedded = layers.Embedding(input_dim=decoder_vocab, output_dim=64)(\n",
        "    decoder_input\n",
        ")\n",
        "\n",
        "# Pass the 2 states to a new LSTM layer, as initial state\n",
        "decoder_output = layers.LSTM(64, name=\"decoder\")(\n",
        "    decoder_embedded, initial_state=encoder_state\n",
        ")\n",
        "output = layers.Dense(10)(decoder_output)\n",
        "\n",
        "model = keras.Model([encoder_input, decoder_input], output)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19e72be49a42"
      },
      "outputs": [],
      "source": [
        "paragraph1 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "paragraph2 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "\n",
        "lstm_layer = layers.LSTM(64, stateful=True)\n",
        "output = lstm_layer(paragraph1)\n",
        "output = lstm_layer(paragraph2)\n",
        "output = lstm_layer(paragraph3)\n",
        "\n",
        "# reset_states() will reset the cached state to the original initial_state.\n",
        "# If no initial_state was provided, zero-states will be used by default.\n",
        "lstm_layer.reset_states()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec7c316b19a1"
      },
      "source": [
        "### RNN State Reuse\n",
        "<a id=\"rnn_state_reuse\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "009c5b393adf"
      },
      "outputs": [],
      "source": [
        "paragraph1 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "paragraph2 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "\n",
        "lstm_layer = layers.LSTM(64, stateful=True)\n",
        "output = lstm_layer(paragraph1)\n",
        "output = lstm_layer(paragraph2)\n",
        "\n",
        "existing_state = lstm_layer.states\n",
        "\n",
        "new_lstm_layer = layers.LSTM(64)\n",
        "new_output = new_lstm_layer(paragraph3, initial_state=existing_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cea1781a0c2"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential()\n",
        "\n",
        "model.add(\n",
        "    layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(5, 10))\n",
        ")\n",
        "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e88aab9e73c7"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "# Each MNIST image batch is a tensor of shape (batch_size, 28, 28).\n",
        "# Each input sequence will be of size (28, 28) (height is treated like time).\n",
        "input_dim = 28\n",
        "\n",
        "units = 64\n",
        "output_size = 10  # labels are from 0 to 9\n",
        "\n",
        "# Build the RNN model\n",
        "def build_model(allow_cudnn_kernel=True):\n",
        "    # CuDNN is only available at the layer level, and not at the cell level.\n",
        "    # This means `LSTM(units)` will use the CuDNN kernel,\n",
        "    # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.\n",
        "    if allow_cudnn_kernel:\n",
        "        # The LSTM layer with default options uses CuDNN.\n",
        "        lstm_layer = keras.layers.LSTM(units, input_shape=(None, input_dim))\n",
        "    else:\n",
        "        # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n",
        "        lstm_layer = keras.layers.RNN(\n",
        "            keras.layers.LSTMCell(units), input_shape=(None, input_dim)\n",
        "        )\n",
        "    model = keras.models.Sequential(\n",
        "        [\n",
        "            lstm_layer,\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.Dense(output_size),\n",
        "        ]\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcde82cb14d6"
      },
      "source": [
        "Let's load the MNIST dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98292f8e71a9"
      },
      "outputs": [],
      "source": [
        "mnist = keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "sample, sample_label = x_train[0], y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "443e5458284f"
      },
      "source": [
        "Let's create a model instance and train it.\n",
        "\n",
        "We choose `sparse_categorical_crossentropy` as the loss function for the model. The\n",
        "output of the model has shape of `[batch_size, 10]`. The target for the model is an\n",
        "integer vector, each of the integer is in the range of 0 to 9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f85b57b010e5"
      },
      "outputs": [],
      "source": [
        "model = build_model(allow_cudnn_kernel=True)\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=\"sgd\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99ea5495e375"
      },
      "source": [
        "Now, let's compare to a model that does not use the CuDNN kernel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4bdff02e617"
      },
      "outputs": [],
      "source": [
        "noncudnn_model = build_model(allow_cudnn_kernel=False)\n",
        "noncudnn_model.set_weights(model.get_weights())\n",
        "noncudnn_model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=\"sgd\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "noncudnn_model.fit(\n",
        "    x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e33c62b6029"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with tf.device(\"CPU:0\"):\n",
        "    cpu_model = build_model(allow_cudnn_kernel=True)\n",
        "    cpu_model.set_weights(model.get_weights())\n",
        "    result = tf.argmax(cpu_model.predict_on_batch(tf.expand_dims(sample, 0)), axis=1)\n",
        "    print(\n",
        "        \"Predicted result is: %s, target result is: %s\" % (result.numpy(), sample_label)\n",
        "    )\n",
        "    plt.imshow(sample, cmap=plt.get_cmap(\"gray\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f78dc4c1c516"
      },
      "source": [
        "### Define a custom cell that supports nested input/output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "199faf57f0c5"
      },
      "source": [
        "See [Making new Layers & Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models/)\n",
        "for details on writing your own layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "451cfd5f0cc4"
      },
      "outputs": [],
      "source": [
        "class NestedCell(keras.layers.Layer):\n",
        "    def __init__(self, unit_1, unit_2, unit_3, **kwargs):\n",
        "        self.unit_1 = unit_1\n",
        "        self.unit_2 = unit_2\n",
        "        self.unit_3 = unit_3\n",
        "        self.state_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]\n",
        "        self.output_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]\n",
        "        super(NestedCell, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]\n",
        "        i1 = input_shapes[0][1]\n",
        "        i2 = input_shapes[1][1]\n",
        "        i3 = input_shapes[1][2]\n",
        "\n",
        "        self.kernel_1 = self.add_weight(\n",
        "            shape=(i1, self.unit_1), initializer=\"uniform\", name=\"kernel_1\"\n",
        "        )\n",
        "        self.kernel_2_3 = self.add_weight(\n",
        "            shape=(i2, i3, self.unit_2, self.unit_3),\n",
        "            initializer=\"uniform\",\n",
        "            name=\"kernel_2_3\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        # inputs should be in [(batch, input_1), (batch, input_2, input_3)]\n",
        "        # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]\n",
        "        input_1, input_2 = tf.nest.flatten(inputs)\n",
        "        s1, s2 = states\n",
        "\n",
        "        output_1 = tf.matmul(input_1, self.kernel_1)\n",
        "        output_2_3 = tf.einsum(\"bij,ijkl->bkl\", input_2, self.kernel_2_3)\n",
        "        state_1 = s1 + output_1\n",
        "        state_2_3 = s2 + output_2_3\n",
        "\n",
        "        output = (output_1, output_2_3)\n",
        "        new_states = (state_1, state_2_3)\n",
        "\n",
        "        return output, new_states\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"unit_1\": self.unit_1, \"unit_2\": unit_2, \"unit_3\": self.unit_3}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51355b4089d2"
      },
      "source": [
        "### Build a RNN model with nested input/output\n",
        "\n",
        "Let's build a Keras model that uses a `keras.layers.RNN` layer and the custom cell\n",
        "we just defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2eba7a248eb"
      },
      "outputs": [],
      "source": [
        "unit_1 = 10\n",
        "unit_2 = 20\n",
        "unit_3 = 30\n",
        "\n",
        "i1 = 32\n",
        "i2 = 64\n",
        "i3 = 32\n",
        "batch_size = 64\n",
        "num_batches = 10\n",
        "timestep = 50\n",
        "\n",
        "cell = NestedCell(unit_1, unit_2, unit_3)\n",
        "rnn = keras.layers.RNN(cell)\n",
        "\n",
        "input_1 = keras.Input((None, i1))\n",
        "input_2 = keras.Input((None, i2, i3))\n",
        "\n",
        "outputs = rnn((input_1, input_2))\n",
        "\n",
        "model = keras.models.Model([input_1, input_2], outputs)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "452a99c63b7c"
      },
      "source": [
        "### Train the model with randomly generated data\n",
        "\n",
        "Since there isn't a good candidate dataset for this model, we use random Numpy data for\n",
        "demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3987993cb7be"
      },
      "outputs": [],
      "source": [
        "input_1_data = np.random.random((batch_size * num_batches, timestep, i1))\n",
        "input_2_data = np.random.random((batch_size * num_batches, timestep, i2, i3))\n",
        "target_1_data = np.random.random((batch_size * num_batches, unit_1))\n",
        "target_2_data = np.random.random((batch_size * num_batches, unit_2, unit_3))\n",
        "input_data = [input_1_data, input_2_data]\n",
        "target_data = [target_1_data, target_2_data]\n",
        "\n",
        "model.fit(input_data, target_data, batch_size=batch_size)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CMPE 258 - Assignment 7 - Part 3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}